{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.cuda = True\n",
    "        self.num_workers = 1\n",
    "        self.features = 32\n",
    "        self.log_dir = \"./logs/waveunet\"\n",
    "        self.dataset_dir = \"/mnt/windaten/Datasets/MUSDB18HQ\"\n",
    "        self.hdf_dir = \"hdf\"\n",
    "        self.checkpoint_dir = \"checkpoints/waveunet\"\n",
    "#         self.load_model = \"waveunet/model\"\n",
    "        self.load_model = None\n",
    "        self.lr = 1e-3\n",
    "        self.min_lr = 5e-5\n",
    "        self.cycles = 2\n",
    "        self.batch_size = 4\n",
    "        self.levels = 6\n",
    "        self.depth = 1\n",
    "        self.sr = 44100\n",
    "        self.channels = 2\n",
    "        self.kernel_size = 5\n",
    "        self.output_size = 2.0\n",
    "        self.strides = 4\n",
    "        self.patience = 20\n",
    "        self.example_freq = 200\n",
    "        self.loss = \"L1\"\n",
    "        self.conv_type = \"gn\"\n",
    "        self.res = \"fixed\"\n",
    "        self.separate = 1\n",
    "        self.feature_growth = \"double\"\n",
    "        self.input = \"audio_examples/Cristina Vane - So Easy/mix.mp3\"\n",
    "        self.output = \"./out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using valid convolutions with 97961 inputs and 88409 outputs\n",
      "move model to gpu\n",
      "model:  DataParallel(\n",
      "  (module): Waveunet(\n",
      "    (waveunets): ModuleDict(\n",
      "      (bass): Module(\n",
      "        (downsampling_blocks): ModuleList(\n",
      "          (0): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(2, 32, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (1): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (2): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (3): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (4): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "        )\n",
      "        (upsampling_blocks): ModuleList(\n",
      "          (0): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (3): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (4): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (bottlenecks): ModuleList(\n",
      "          (0): ConvLayer(\n",
      "            (filter): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n",
      "            (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
      "          )\n",
      "        )\n",
      "        (output_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (drums): Module(\n",
      "        (downsampling_blocks): ModuleList(\n",
      "          (0): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(2, 32, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (1): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (2): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (3): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (4): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "        )\n",
      "        (upsampling_blocks): ModuleList(\n",
      "          (0): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (3): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (4): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (bottlenecks): ModuleList(\n",
      "          (0): ConvLayer(\n",
      "            (filter): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n",
      "            (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
      "          )\n",
      "        )\n",
      "        (output_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (other): Module(\n",
      "        (downsampling_blocks): ModuleList(\n",
      "          (0): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(2, 32, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (1): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (2): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (3): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (4): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "        )\n",
      "        (upsampling_blocks): ModuleList(\n",
      "          (0): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (3): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (4): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (bottlenecks): ModuleList(\n",
      "          (0): ConvLayer(\n",
      "            (filter): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n",
      "            (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
      "          )\n",
      "        )\n",
      "        (output_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (vocals): Module(\n",
      "        (downsampling_blocks): ModuleList(\n",
      "          (0): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(2, 32, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (1): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (2): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (3): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "          (4): DownsamplingBlock(\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 1024, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (downconv): Resample1d()\n",
      "          )\n",
      "        )\n",
      "        (upsampling_blocks): ModuleList(\n",
      "          (0): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(1024, 512, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(64, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(512, 256, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(256, 128, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(16, 128, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (3): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(128, 64, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (4): UpsamplingBlock(\n",
      "            (upconv): Resample1d()\n",
      "            (pre_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "            (post_shortcut_convs): ModuleList(\n",
      "              (0): ConvLayer(\n",
      "                (filter): Conv1d(64, 32, kernel_size=(5,), stride=(1,))\n",
      "                (norm): GroupNorm(4, 32, eps=1e-05, affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (bottlenecks): ModuleList(\n",
      "          (0): ConvLayer(\n",
      "            (filter): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,))\n",
      "            (norm): GroupNorm(128, 1024, eps=1e-05, affine=True)\n",
      "          )\n",
      "        )\n",
      "        (output_conv): Conv1d(32, 2, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "parameter count:  70148232\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "from data import get_musdb_folds, SeparationDataset, random_amplify, crop\n",
    "# from test import evaluate, validate\n",
    "from waveunet import Waveunet\n",
    "\n",
    "args = Parameters()\n",
    "\n",
    "INSTRUMENTS = [\"bass\", \"drums\", \"other\", \"vocals\"]\n",
    "NUM_INSTRUMENTS = len(INSTRUMENTS)\n",
    "\n",
    "#torch.backends.cudnn.benchmark=True # This makes dilated conv much faster for CuDNN 7.5\n",
    "\n",
    "# MODEL\n",
    "num_features = [args.features*i for i in range(1, args.levels+1)] if args.feature_growth == \"add\" else \\\n",
    "               [args.features*2**i for i in range(0, args.levels)]\n",
    "target_outputs = int(args.output_size * args.sr)\n",
    "model = Waveunet(args.channels, num_features, args.channels, INSTRUMENTS, kernel_size=args.kernel_size,\n",
    "                 target_output_size=target_outputs, depth=args.depth, strides=args.strides,\n",
    "                 conv_type=args.conv_type, res=args.res, separate=args.separate)\n",
    "\n",
    "if args.cuda:\n",
    "    model = utils.DataParallel(model)\n",
    "    print(\"move model to gpu\")\n",
    "    model.cuda()\n",
    "    \n",
    "print('model: ', model)\n",
    "print('parameter count: ', str(sum(p.numel() for p in model.parameters())))\n",
    "\n",
    "writer = SummaryWriter(args.log_dir)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.waveunets[[k for k in model.waveunets.keys()][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module.upsampling_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "import museval\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from utils import compute_loss\n",
    "\n",
    "def predict(audio, model):\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        is_cuda = audio.is_cuda()\n",
    "        audio = audio.detach().cpu().numpy()\n",
    "        return_mode = \"pytorch\"\n",
    "    else:\n",
    "        return_mode = \"numpy\"\n",
    "\n",
    "    expected_outputs = audio.shape[1]\n",
    "\n",
    "    # Pad input if it is not divisible in length by the frame shift number\n",
    "    output_shift = model.shapes[\"output_frames\"]\n",
    "    pad_back = audio.shape[1] % output_shift\n",
    "    pad_back = 0 if pad_back == 0 else output_shift - pad_back\n",
    "    if pad_back > 0:\n",
    "        audio = np.pad(audio, [(0,0), (0, pad_back)], mode=\"constant\", constant_values=0.0)\n",
    "\n",
    "    target_outputs = audio.shape[1]\n",
    "    outputs = {key: np.zeros(audio.shape, np.float32) for key in model.instruments}\n",
    "\n",
    "    # Pad mixture across time at beginning and end so that neural network can make prediction at the beginning and end of signal\n",
    "    pad_front_context = model.shapes[\"output_start_frame\"]\n",
    "    pad_back_context = model.shapes[\"input_frames\"] - model.shapes[\"output_end_frame\"]\n",
    "    audio = np.pad(audio, [(0,0), (pad_front_context, pad_back_context)], mode=\"constant\", constant_values=0.0)\n",
    "\n",
    "    # Iterate over mixture magnitudes, fetch network prediction\n",
    "    with torch.no_grad():\n",
    "        for target_start_pos in range(0, target_outputs, model.shapes[\"output_frames\"]):\n",
    "\n",
    "            # Prepare mixture excerpt by selecting time interval\n",
    "            curr_input = audio[:, target_start_pos:target_start_pos + model.shapes[\"input_frames\"]] # Since audio was front-padded input of [targetpos:targetpos+inputframes] actually predicts [targetpos:targetpos+outputframes] target range\n",
    "\n",
    "            # Convert to Pytorch tensor for model prediction\n",
    "            curr_input = torch.from_numpy(curr_input).unsqueeze(0)\n",
    "\n",
    "            # Predict\n",
    "            for key, curr_targets in utils.compute_output(model, curr_input).items():\n",
    "                outputs[key][:,target_start_pos:target_start_pos+model.shapes[\"output_frames\"]] = curr_targets.squeeze(0).cpu().numpy()\n",
    "\n",
    "    # Crop to expected length (since we padded to handle the frame shift)\n",
    "    outputs = {key : outputs[key][:,:expected_outputs] for key in outputs.keys()}\n",
    "\n",
    "    if return_mode == \"pytorch\":\n",
    "        outputs = torch.from_numpy(outputs)\n",
    "        if is_cuda:\n",
    "            outputs = outputs.cuda()\n",
    "    return outputs\n",
    "\n",
    "def predict_song(args, audio_path, model):\n",
    "    model.eval()\n",
    "\n",
    "    # Load mixture in original sampling rate\n",
    "    mix_audio, mix_sr = utils.load(audio_path, sr=None, mono=False)\n",
    "    mix_channels = mix_audio.shape[0]\n",
    "    mix_len = mix_audio.shape[1]\n",
    "\n",
    "    # Adapt mixture channels to required input channels\n",
    "    if args.channels == 1:\n",
    "        mix_audio = np.mean(mix_audio, axis=0, keepdims=True)\n",
    "    else:\n",
    "        if mix_channels == 1: # Duplicate channels if input is mono but model is stereo\n",
    "            mix_audio = np.tile(mix_audio, [args.channels, 1])\n",
    "        else:\n",
    "            assert(mix_channels == args.channels)\n",
    "\n",
    "    # resample to model sampling rate\n",
    "    mix_audio = utils.resample(mix_audio, mix_sr, args.sr)\n",
    "\n",
    "    sources = predict(mix_audio, model)\n",
    "\n",
    "    # Resample back to mixture sampling rate in case we had model on different sampling rate\n",
    "    sources = {key : utils.resample(sources[key], args.sr, mix_sr) for key in sources.keys()}\n",
    "\n",
    "    # In case we had to pad the mixture at the end, or we have a few samples too many due to inconsistent down- and upsamṕling, remove those samples from source prediction now\n",
    "    for key in sources.keys():\n",
    "        diff = sources[key].shape[1] - mix_len\n",
    "        if diff > 0:\n",
    "            print(\"WARNING: Cropping \" + str(diff) + \" samples\")\n",
    "            sources[key] = sources[key][:, :-diff]\n",
    "        elif diff < 0:\n",
    "            print(\"WARNING: Padding output by \" + str(diff) + \" samples\")\n",
    "            sources[key] = np.pad(sources[key], [(0,0), (0, -diff)], \"constant\", 0.0)\n",
    "\n",
    "        # Adapt channels\n",
    "        if mix_channels > args.channels:\n",
    "            assert(args.channels == 1)\n",
    "            # Duplicate mono predictions\n",
    "            sources[key] = np.tile(sources[key], [mix_channels, 1])\n",
    "        elif mix_channels < args.channels:\n",
    "            assert(mix_channels == 1)\n",
    "            # Reduce model output to mono\n",
    "            sources[key] = np.mean(sources[key], axis=0, keepdims=True)\n",
    "\n",
    "        sources[key] = np.asfortranarray(sources[key]) # So librosa does not complain if we want to save it\n",
    "\n",
    "    return sources\n",
    "\n",
    "def evaluate(args, dataset, model, instruments):\n",
    "    perfs = list()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for example in dataset:\n",
    "            print(\"Evaluating \" + example[\"mix\"])\n",
    "\n",
    "            # Load source references in their original sr and channel number\n",
    "            target_sources = np.stack([utils.load(example[instrument], sr=None, mono=False)[0].T for instrument in instruments])\n",
    "\n",
    "            # Predict using mixture\n",
    "            pred_sources = predict_song(args, example[\"mix\"], model)\n",
    "            pred_sources = np.stack([pred_sources[key].T for key in instruments])\n",
    "\n",
    "            # Evaluate\n",
    "            SDR, ISR, SIR, SAR, _ = museval.metrics.bss_eval(target_sources, pred_sources)\n",
    "            song = {}\n",
    "            for idx, name in enumerate(instruments):\n",
    "                song[name] = {\"SDR\" : SDR[idx], \"ISR\" : ISR[idx], \"SIR\" : SIR[idx], \"SAR\" : SAR[idx]}\n",
    "            perfs.append(song)\n",
    "\n",
    "    return perfs\n",
    "\n",
    "\n",
    "def validate(args, model, criterion, test_data):\n",
    "    # PREPARE DATA\n",
    "    dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                             batch_size=args.batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=args.num_workers)\n",
    "\n",
    "    # VALIDATE\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with tqdm(total=len(test_data) // args.batch_size) as pbar, torch.no_grad():\n",
    "        for example_num, (x, targets) in enumerate(dataloader):\n",
    "            if args.cuda:\n",
    "                x = x.cuda()\n",
    "                for k in list(targets.keys()):\n",
    "                    targets[k] = targets[k].cuda()\n",
    "\n",
    "            _, avg_loss = compute_loss(model, x, targets, criterion)\n",
    "\n",
    "            total_loss += (1. / float(example_num + 1)) * (avg_loss - total_loss)\n",
    "\n",
    "            pbar.set_description(\"Current loss: \" + str(total_loss))\n",
    "            pbar.update(1)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveunet.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from waveunet_utils import crop, Resample1d, ConvLayer\n",
    "\n",
    "class UpsamplingBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_shortcut, n_outputs, kernel_size, stride, depth, conv_type, res):\n",
    "        super(UpsamplingBlock, self).__init__()\n",
    "        assert(stride > 1)\n",
    "\n",
    "        # CONV 1 for UPSAMPLING\n",
    "        if res == \"fixed\":\n",
    "            self.upconv = Resample1d(n_inputs, 15, stride, transpose=True)\n",
    "        else:\n",
    "            self.upconv = ConvLayer(n_inputs, n_inputs, kernel_size, stride, conv_type, transpose=True)\n",
    "\n",
    "        self.pre_shortcut_convs = nn.ModuleList([ConvLayer(n_inputs, n_outputs, kernel_size, 1, conv_type)] +\n",
    "                                                [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
    "\n",
    "        # CONVS to combine high- with low-level information (from shortcut)\n",
    "        self.post_shortcut_convs = nn.ModuleList([ConvLayer(n_outputs + n_shortcut, n_outputs, kernel_size, 1, conv_type)] +\n",
    "                                                 [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
    "\n",
    "    def forward(self, x, shortcut):\n",
    "        # UPSAMPLE HIGH-LEVEL FEATURES\n",
    "        upsampled = self.upconv(x)\n",
    "\n",
    "        for conv in self.pre_shortcut_convs:\n",
    "            upsampled = conv(upsampled)\n",
    "\n",
    "        # Prepare shortcut connection\n",
    "        combined = crop(shortcut, upsampled)\n",
    "\n",
    "        # Combine high- and low-level features\n",
    "        for conv in self.post_shortcut_convs:\n",
    "            combined = conv(torch.cat([combined, crop(upsampled, combined)], dim=1))\n",
    "        return combined\n",
    "\n",
    "    def get_output_size(self, input_size):\n",
    "        curr_size = self.upconv.get_output_size(input_size)\n",
    "\n",
    "        # Upsampling convs\n",
    "        for conv in self.pre_shortcut_convs:\n",
    "            curr_size = conv.get_output_size(curr_size)\n",
    "\n",
    "        # Combine convolutions\n",
    "        for conv in self.post_shortcut_convs:\n",
    "            curr_size = conv.get_output_size(curr_size)\n",
    "\n",
    "        return curr_size\n",
    "\n",
    "class DownsamplingBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_shortcut, n_outputs, kernel_size, stride, depth, conv_type, res):\n",
    "        super(DownsamplingBlock, self).__init__()\n",
    "        assert(stride > 1)\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        # CONV 1\n",
    "        self.pre_shortcut_convs = nn.ModuleList([ConvLayer(n_inputs, n_shortcut, kernel_size, 1, conv_type)] +\n",
    "                                                [ConvLayer(n_shortcut, n_shortcut, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
    "\n",
    "        self.post_shortcut_convs = nn.ModuleList([ConvLayer(n_shortcut, n_outputs, kernel_size, 1, conv_type)] +\n",
    "                                                 [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in\n",
    "                                                  range(depth - 1)])\n",
    "\n",
    "        # CONV 2 with decimation\n",
    "        if res == \"fixed\":\n",
    "            self.downconv = Resample1d(n_outputs, 15, stride) # Resampling with fixed-size sinc lowpass filter\n",
    "        else:\n",
    "            self.downconv = ConvLayer(n_outputs, n_outputs, kernel_size, stride, conv_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PREPARING SHORTCUT FEATURES\n",
    "        shortcut = x\n",
    "        for conv in self.pre_shortcut_convs:\n",
    "            shortcut = conv(shortcut)\n",
    "\n",
    "        # PREPARING FOR DOWNSAMPLING\n",
    "        out = shortcut\n",
    "        for conv in self.post_shortcut_convs:\n",
    "            out = conv(out)\n",
    "\n",
    "        # DOWNSAMPLING\n",
    "        out = self.downconv(out)\n",
    "\n",
    "        return out, shortcut\n",
    "\n",
    "    def get_input_size(self, output_size):\n",
    "        curr_size = self.downconv.get_input_size(output_size)\n",
    "\n",
    "        for conv in reversed(self.post_shortcut_convs):\n",
    "            curr_size = conv.get_input_size(curr_size)\n",
    "\n",
    "        for conv in reversed(self.pre_shortcut_convs):\n",
    "            curr_size = conv.get_input_size(curr_size)\n",
    "        return curr_size\n",
    "\n",
    "class Waveunet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, num_outputs, instruments, kernel_size, target_output_size, conv_type, res, separate=False, depth=1, strides=2):\n",
    "        super(Waveunet, self).__init__()\n",
    "\n",
    "        self.num_levels = len(num_channels)\n",
    "        self.strides = strides\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.depth = depth\n",
    "        self.instruments = instruments\n",
    "        self.separate = separate\n",
    "\n",
    "        # Only odd filter kernels allowed\n",
    "        assert(kernel_size % 2 == 1)\n",
    "\n",
    "        self.waveunets = nn.ModuleDict()\n",
    "\n",
    "        model_list = instruments if separate else [\"ALL\"]\n",
    "        # Create a model for each source if we separate sources separately, otherwise only one (model_list=[\"ALL\"])\n",
    "        for instrument in model_list:\n",
    "            module = nn.Module()\n",
    "\n",
    "            module.downsampling_blocks = nn.ModuleList()\n",
    "            module.upsampling_blocks = nn.ModuleList()\n",
    "\n",
    "            for i in range(self.num_levels - 1):\n",
    "                in_ch = num_inputs if i == 0 else num_channels[i]\n",
    "\n",
    "                module.downsampling_blocks.append(\n",
    "                    DownsamplingBlock(in_ch, num_channels[i], num_channels[i+1], kernel_size, strides, depth, conv_type, res))\n",
    "\n",
    "            for i in range(0, self.num_levels - 1):\n",
    "                module.upsampling_blocks.append(\n",
    "                    UpsamplingBlock(num_channels[-1-i], num_channels[-2-i], num_channels[-2-i], kernel_size, strides, depth, conv_type, res))\n",
    "\n",
    "            module.bottlenecks = nn.ModuleList(\n",
    "                [ConvLayer(num_channels[-1], num_channels[-1], kernel_size, 1, conv_type) for _ in range(depth)])\n",
    "\n",
    "            # Output conv\n",
    "            outputs = num_outputs if separate else num_outputs * len(instruments)\n",
    "            module.output_conv = nn.Conv1d(num_channels[0], outputs, 1)\n",
    "\n",
    "            self.waveunets[instrument] = module\n",
    "\n",
    "        self.set_output_size(target_output_size)\n",
    "\n",
    "    def set_output_size(self, target_output_size):\n",
    "        self.target_output_size = target_output_size\n",
    "\n",
    "        self.input_size, self.output_size = self.check_padding(target_output_size)\n",
    "        print(\"Using valid convolutions with \" + str(self.input_size) + \" inputs and \" + str(self.output_size) + \" outputs\")\n",
    "\n",
    "        assert((self.input_size - self.output_size) % 2 == 0)\n",
    "        self.shapes = {\"output_start_frame\" : (self.input_size - self.output_size) // 2,\n",
    "                       \"output_end_frame\" : (self.input_size - self.output_size) // 2 + self.output_size,\n",
    "                       \"output_frames\" : self.output_size,\n",
    "                       \"input_frames\" : self.input_size}\n",
    "\n",
    "    def check_padding(self, target_output_size):\n",
    "        # Ensure number of outputs covers a whole number of cycles so each output in the cycle is weighted equally during training\n",
    "        bottleneck = 1\n",
    "\n",
    "        while True:\n",
    "            out = self.check_padding_for_bottleneck(bottleneck, target_output_size)\n",
    "            if out is not False:\n",
    "                return out\n",
    "            bottleneck += 1\n",
    "\n",
    "    def check_padding_for_bottleneck(self, bottleneck, target_output_size):\n",
    "        module = self.waveunets[[k for k in self.waveunets.keys()][0]]\n",
    "        try:\n",
    "            curr_size = bottleneck\n",
    "            for idx, block in enumerate(module.upsampling_blocks):\n",
    "                curr_size = block.get_output_size(curr_size)\n",
    "            output_size = curr_size\n",
    "\n",
    "            # Bottleneck-Conv\n",
    "            curr_size = bottleneck\n",
    "            for block in reversed(module.bottlenecks):\n",
    "                curr_size = block.get_input_size(curr_size)\n",
    "            for idx, block in enumerate(reversed(module.downsampling_blocks)):\n",
    "                curr_size = block.get_input_size(curr_size)\n",
    "\n",
    "            assert(output_size >= target_output_size)\n",
    "            return curr_size, output_size\n",
    "        except AssertionError as e:\n",
    "            return False\n",
    "\n",
    "    def forward_module(self, x, module):\n",
    "        '''\n",
    "        A forward pass through a single Wave-U-Net (multiple Wave-U-Nets might be used, one for each source)\n",
    "        :param x: Input mix\n",
    "        :param module: Network module to be used for prediction\n",
    "        :return: Source estimates\n",
    "        '''\n",
    "        shortcuts = []\n",
    "        out = x\n",
    "\n",
    "        # DOWNSAMPLING BLOCKS\n",
    "        for block in module.downsampling_blocks:\n",
    "            out, short = block(out)\n",
    "            shortcuts.append(short)\n",
    "\n",
    "        # BOTTLENECK CONVOLUTION\n",
    "        for conv in module.bottlenecks:\n",
    "            out = conv(out)\n",
    "\n",
    "        # UPSAMPLING BLOCKS\n",
    "        for idx, block in enumerate(module.upsampling_blocks):\n",
    "            out = block(out, shortcuts[-1 - idx])\n",
    "\n",
    "        # OUTPUT CONV\n",
    "        out = module.output_conv(out)\n",
    "        if not self.training:  # At test time clip predictions to valid amplitude range\n",
    "            out = out.clamp(min=-1.0, max=1.0)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, inst=None):\n",
    "        curr_input_size = x.shape[-1]\n",
    "        assert(curr_input_size == self.input_size) # User promises to feed the proper input himself, to get the pre-calculated (NOT the originally desired) output size\n",
    "\n",
    "        if self.separate:\n",
    "            return {inst : self.forward_module(x, self.waveunets[inst])}\n",
    "        else:\n",
    "            assert(len(self.waveunets) == 1)\n",
    "            out = self.forward_module(x, self.waveunets[\"ALL\"])\n",
    "\n",
    "            out_dict = {}\n",
    "            for idx, inst in enumerate(self.instruments):\n",
    "                out_dict[inst] = out[:, idx * self.num_outputs:(idx + 1) * self.num_outputs]\n",
    "            return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "args = Parameters()\n",
    "os.makedirs(args.output, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/waveunet/lib/python3.6/site-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1323000)\n"
     ]
    }
   ],
   "source": [
    "# fun play\n",
    "audio_path = args.input\n",
    "mix_audio, mix_sr = utils.load(audio_path, sr=None, mono=False)\n",
    "mix_channels = mix_audio.shape[0]\n",
    "mix_len = mix_audio.shape[1]\n",
    "print(mix_audio.shape)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using valid convolutions with 97961 inputs and 88409 outputs\n",
      "move model to gpu\n",
      "Loading model from checkpoint waveunet/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/waveunet/lib/python3.6/site-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import utils\n",
    "\n",
    "# from test import predict_song\n",
    "# import test\n",
    "from waveunet import Waveunet\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--cuda', action='store_true',\n",
    "#                     help='use CUDA (default: False)')\n",
    "# parser.add_argument('--features', type=int, default=32,\n",
    "#                     help='# of feature channels per layer')\n",
    "# parser.add_argument('--load_model', type=str,\n",
    "#                     help='Reload a previously trained model')\n",
    "# parser.add_argument('--batch_size', type=int, default=4,\n",
    "#                     help=\"Batch size\")\n",
    "# parser.add_argument('--levels', type=int, default=6,\n",
    "#                     help=\"Number DS/US blocks\")\n",
    "# parser.add_argument('--depth', type=int, default=1,\n",
    "#                     help=\"Number of convs per block\")\n",
    "# parser.add_argument('--sr', type=int, default=44100,\n",
    "#                     help=\"Sampling rate\")\n",
    "# parser.add_argument('--channels', type=int, default=2,\n",
    "#                     help=\"Number of input audio channels\")\n",
    "# parser.add_argument('--kernel_size', type=int, default=5,\n",
    "#                     help=\"Filter width of kernels. Has to be an odd number\")\n",
    "# parser.add_argument('--output_size', type=float, default=2.0,\n",
    "#                     help=\"Output duration\")\n",
    "# parser.add_argument('--strides', type=int, default=4,\n",
    "#                     help=\"Strides in Waveunet\")\n",
    "# parser.add_argument('--conv_type', type=str, default=\"gn\",\n",
    "#                     help=\"Type of convolution (normal, BN-normalised, GN-normalised): normal/bn/gn\")\n",
    "# parser.add_argument('--res', type=str, default=\"fixed\",\n",
    "#                     help=\"Resampling strategy: fixed sinc-based lowpass filtering or learned conv layer: fixed/learned\")\n",
    "# parser.add_argument('--separate', type=int, default=1,\n",
    "#                     help=\"Train separate model for each source (1) or only one (0)\")\n",
    "# parser.add_argument('--feature_growth', type=str, default=\"double\",\n",
    "#                     help=\"How the features in each layer should grow, either (add) the initial number of features each time, or multiply by 2 (double)\")\n",
    "\n",
    "# parser.add_argument('--input', type=str, default=os.path.join(\"audio_examples\", \"Cristina Vane - So Easy\", \"mix.mp3\"),\n",
    "#                     help=\"Path to input mixture to be separated\")\n",
    "# parser.add_argument('--output', type=str, default=None, help=\"Output path (same folder as input path if not set)\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "INSTRUMENTS = [\"bass\", \"drums\", \"other\", \"vocals\"]\n",
    "NUM_INSTRUMENTS = len(INSTRUMENTS)\n",
    "\n",
    "# MODEL\n",
    "num_features = [args.features*i for i in range(1, args.levels+1)] if args.feature_growth == \"add\" else \\\n",
    "               [args.features*2**i for i in range(0, args.levels)]\n",
    "target_outputs = int(args.output_size * args.sr)\n",
    "model = Waveunet(args.channels, num_features, args.channels, INSTRUMENTS, kernel_size=args.kernel_size,\n",
    "                 target_output_size=target_outputs, depth=args.depth, strides=args.strides,\n",
    "                 conv_type=args.conv_type, res=args.res, separate=args.separate)\n",
    "\n",
    "if args.cuda:\n",
    "    model = utils.DataParallel(model)\n",
    "    print(\"move model to gpu\")\n",
    "    model.cuda()\n",
    "\n",
    "print(\"Loading model from checkpoint \" + str(args.load_model))\n",
    "state = utils.load_model(model, None, args.load_model)\n",
    "\n",
    "preds = predict_song(args, args.input, model)\n",
    "\n",
    "output_folder = os.path.dirname(args.input) if args.output is None else args.output\n",
    "for inst in preds.keys():\n",
    "    utils.write_wav(os.path.join(output_folder, os.path.basename(args.input) + \"_\" + inst + \".wav\"), preds[inst], args.sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88409"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.shapes[\"output_frames\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4776 4776\n",
      "97961 93185\n",
      "9552\n"
     ]
    }
   ],
   "source": [
    "pad_front_context = model.shapes[\"output_start_frame\"]\n",
    "pad_back_context = model.shapes[\"input_frames\"] - model.shapes[\"output_end_frame\"]\n",
    "print(pad_front_context, pad_back_context)\n",
    "print(model.shapes[\"input_frames\"] , model.shapes[\"output_end_frame\"])\n",
    "print(model.shapes[\"input_frames\"] - model.shapes[\"output_frames\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1323000)\n",
      "(2, 1326135)\n",
      "(2, 1335687)\n",
      "9552\n"
     ]
    }
   ],
   "source": [
    "audio = mix_audio\n",
    "print(audio.shape)\n",
    "expected_outputs = audio.shape[1]\n",
    "\n",
    "# Pad input if it is not divisible in length by the frame shift number\n",
    "output_shift = model.shapes[\"output_frames\"]\n",
    "pad_back = audio.shape[1] % output_shift\n",
    "pad_back = 0 if pad_back == 0 else output_shift - pad_back\n",
    "if pad_back > 0:\n",
    "    audio = np.pad(audio, [(0,0), (0, pad_back)], mode=\"constant\", constant_values=0.0)\n",
    "print(audio.shape)\n",
    "target_outputs = audio.shape[1]\n",
    "outputs = {key: np.zeros(audio.shape, np.float32) for key in model.instruments}\n",
    "\n",
    "# Pad mixture across time at beginning and end so that neural network can make prediction at the beginning and end of signal\n",
    "pad_front_context = model.shapes[\"output_start_frame\"]\n",
    "pad_back_context = model.shapes[\"input_frames\"] - model.shapes[\"output_end_frame\"]\n",
    "audio = np.pad(audio, [(0,0), (pad_front_context, pad_back_context)], mode=\"constant\", constant_values=0.0)\n",
    "print(audio.shape)\n",
    "print(audio.shape[1]% output_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
