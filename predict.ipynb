{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "import museval\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from utils import compute_loss\n",
    "\n",
    "def predict(audio, model):\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        is_cuda = audio.is_cuda()\n",
    "        audio = audio.detach().cpu().numpy()\n",
    "        return_mode = \"pytorch\"\n",
    "    else:\n",
    "        return_mode = \"numpy\"\n",
    "\n",
    "    expected_outputs = audio.shape[1]\n",
    "\n",
    "    # Pad input if it is not divisible in length by the frame shift number\n",
    "    output_shift = model.shapes[\"output_frames\"]\n",
    "    pad_back = audio.shape[1] % output_shift\n",
    "    pad_back = 0 if pad_back == 0 else output_shift - pad_back\n",
    "    if pad_back > 0:\n",
    "        audio = np.pad(audio, [(0,0), (0, pad_back)], mode=\"constant\", constant_values=0.0)\n",
    "\n",
    "    target_outputs = audio.shape[1]\n",
    "    outputs = {key: np.zeros(audio.shape, np.float32) for key in model.instruments}\n",
    "\n",
    "    # Pad mixture across time at beginning and end so that neural network can make prediction at the beginning and end of signal\n",
    "    pad_front_context = model.shapes[\"output_start_frame\"]\n",
    "    pad_back_context = model.shapes[\"input_frames\"] - model.shapes[\"output_end_frame\"]\n",
    "    audio = np.pad(audio, [(0,0), (pad_front_context, pad_back_context)], mode=\"constant\", constant_values=0.0)\n",
    "\n",
    "    # Iterate over mixture magnitudes, fetch network prediction\n",
    "    with torch.no_grad():\n",
    "        for target_start_pos in range(0, target_outputs, model.shapes[\"output_frames\"]):\n",
    "\n",
    "            # Prepare mixture excerpt by selecting time interval\n",
    "            curr_input = audio[:, target_start_pos:target_start_pos + model.shapes[\"input_frames\"]] # Since audio was front-padded input of [targetpos:targetpos+inputframes] actually predicts [targetpos:targetpos+outputframes] target range\n",
    "\n",
    "            # Convert to Pytorch tensor for model prediction\n",
    "            curr_input = torch.from_numpy(curr_input).unsqueeze(0)\n",
    "\n",
    "            # Predict\n",
    "            for key, curr_targets in utils.compute_output(model, curr_input).items():\n",
    "                outputs[key][:,target_start_pos:target_start_pos+model.shapes[\"output_frames\"]] = curr_targets.squeeze(0).cpu().numpy()\n",
    "\n",
    "    # Crop to expected length (since we padded to handle the frame shift)\n",
    "    outputs = {key : outputs[key][:,:expected_outputs] for key in outputs.keys()}\n",
    "\n",
    "    if return_mode == \"pytorch\":\n",
    "        outputs = torch.from_numpy(outputs)\n",
    "        if is_cuda:\n",
    "            outputs = outputs.cuda()\n",
    "    return outputs\n",
    "\n",
    "def predict_song(args, audio_path, model):\n",
    "    model.eval()\n",
    "\n",
    "    # Load mixture in original sampling rate\n",
    "    mix_audio, mix_sr = utils.load(audio_path, sr=None, mono=False)\n",
    "    mix_channels = mix_audio.shape[0]\n",
    "    mix_len = mix_audio.shape[1]\n",
    "\n",
    "    # Adapt mixture channels to required input channels\n",
    "    if args.channels == 1:\n",
    "        mix_audio = np.mean(mix_audio, axis=0, keepdims=True)\n",
    "    else:\n",
    "        if mix_channels == 1: # Duplicate channels if input is mono but model is stereo\n",
    "            mix_audio = np.tile(mix_audio, [args.channels, 1])\n",
    "        else:\n",
    "            assert(mix_channels == args.channels)\n",
    "\n",
    "    # resample to model sampling rate\n",
    "    mix_audio = utils.resample(mix_audio, mix_sr, args.sr)\n",
    "\n",
    "    sources = predict(mix_audio, model)\n",
    "\n",
    "    # Resample back to mixture sampling rate in case we had model on different sampling rate\n",
    "    sources = {key : utils.resample(sources[key], args.sr, mix_sr) for key in sources.keys()}\n",
    "\n",
    "    # In case we had to pad the mixture at the end, or we have a few samples too many due to inconsistent down- and upsamá¹•ling, remove those samples from source prediction now\n",
    "    for key in sources.keys():\n",
    "        diff = sources[key].shape[1] - mix_len\n",
    "        if diff > 0:\n",
    "            print(\"WARNING: Cropping \" + str(diff) + \" samples\")\n",
    "            sources[key] = sources[key][:, :-diff]\n",
    "        elif diff < 0:\n",
    "            print(\"WARNING: Padding output by \" + str(diff) + \" samples\")\n",
    "            sources[key] = np.pad(sources[key], [(0,0), (0, -diff)], \"constant\", 0.0)\n",
    "\n",
    "        # Adapt channels\n",
    "        if mix_channels > args.channels:\n",
    "            assert(args.channels == 1)\n",
    "            # Duplicate mono predictions\n",
    "            sources[key] = np.tile(sources[key], [mix_channels, 1])\n",
    "        elif mix_channels < args.channels:\n",
    "            assert(mix_channels == 1)\n",
    "            # Reduce model output to mono\n",
    "            sources[key] = np.mean(sources[key], axis=0, keepdims=True)\n",
    "\n",
    "        sources[key] = np.asfortranarray(sources[key]) # So librosa does not complain if we want to save it\n",
    "\n",
    "    return sources\n",
    "\n",
    "def evaluate(args, dataset, model, instruments):\n",
    "    perfs = list()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for example in dataset:\n",
    "            print(\"Evaluating \" + example[\"mix\"])\n",
    "\n",
    "            # Load source references in their original sr and channel number\n",
    "            target_sources = np.stack([utils.load(example[instrument], sr=None, mono=False)[0].T for instrument in instruments])\n",
    "\n",
    "            # Predict using mixture\n",
    "            pred_sources = predict_song(args, example[\"mix\"], model)\n",
    "            pred_sources = np.stack([pred_sources[key].T for key in instruments])\n",
    "\n",
    "            # Evaluate\n",
    "            SDR, ISR, SIR, SAR, _ = museval.metrics.bss_eval(target_sources, pred_sources)\n",
    "            song = {}\n",
    "            for idx, name in enumerate(instruments):\n",
    "                song[name] = {\"SDR\" : SDR[idx], \"ISR\" : ISR[idx], \"SIR\" : SIR[idx], \"SAR\" : SAR[idx]}\n",
    "            perfs.append(song)\n",
    "\n",
    "    return perfs\n",
    "\n",
    "\n",
    "def validate(args, model, criterion, test_data):\n",
    "    # PREPARE DATA\n",
    "    dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                             batch_size=args.batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=args.num_workers)\n",
    "\n",
    "    # VALIDATE\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with tqdm(total=len(test_data) // args.batch_size) as pbar, torch.no_grad():\n",
    "        for example_num, (x, targets) in enumerate(dataloader):\n",
    "            if args.cuda:\n",
    "                x = x.cuda()\n",
    "                for k in list(targets.keys()):\n",
    "                    targets[k] = targets[k].cuda()\n",
    "\n",
    "            _, avg_loss = compute_loss(model, x, targets, criterion)\n",
    "\n",
    "            total_loss += (1. / float(example_num + 1)) * (avg_loss - total_loss)\n",
    "\n",
    "            pbar.set_description(\"Current loss: \" + str(total_loss))\n",
    "            pbar.update(1)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.cuda = True\n",
    "        self.features = 32\n",
    "        self.load_model = \"waveunet/model\"\n",
    "        self.batch_size = 4\n",
    "        self.levels = 6\n",
    "        self.depth = 1\n",
    "        self.sr = 44100\n",
    "        self.channels = 2\n",
    "        self.kernel_size = 5\n",
    "        self.output_size = 2.0\n",
    "        self.strides = 4\n",
    "        self.conv_type = \"gn\"\n",
    "        self.res = \"fixed\"\n",
    "        self.separate = 1\n",
    "        self.feature_growth = \"double\"\n",
    "        self.input = \"audio_examples/Cristina Vane - So Easy/mix.mp3\"\n",
    "        self.output = \"./out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "args = Parameters()\n",
    "os.makedirs(args.output, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/waveunet/lib/python3.6/site-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1323000)\n"
     ]
    }
   ],
   "source": [
    "# fun play\n",
    "audio_path = args.input\n",
    "mix_audio, mix_sr = utils.load(audio_path, sr=None, mono=False)\n",
    "mix_channels = mix_audio.shape[0]\n",
    "mix_len = mix_audio.shape[1]\n",
    "print(mix_audio.shape)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using valid convolutions with 97961 inputs and 88409 outputs\n",
      "move model to gpu\n",
      "Loading model from checkpoint waveunet/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/waveunet/lib/python3.6/site-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import utils\n",
    "\n",
    "# from test import predict_song\n",
    "# import test\n",
    "from waveunet import Waveunet\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--cuda', action='store_true',\n",
    "#                     help='use CUDA (default: False)')\n",
    "# parser.add_argument('--features', type=int, default=32,\n",
    "#                     help='# of feature channels per layer')\n",
    "# parser.add_argument('--load_model', type=str,\n",
    "#                     help='Reload a previously trained model')\n",
    "# parser.add_argument('--batch_size', type=int, default=4,\n",
    "#                     help=\"Batch size\")\n",
    "# parser.add_argument('--levels', type=int, default=6,\n",
    "#                     help=\"Number DS/US blocks\")\n",
    "# parser.add_argument('--depth', type=int, default=1,\n",
    "#                     help=\"Number of convs per block\")\n",
    "# parser.add_argument('--sr', type=int, default=44100,\n",
    "#                     help=\"Sampling rate\")\n",
    "# parser.add_argument('--channels', type=int, default=2,\n",
    "#                     help=\"Number of input audio channels\")\n",
    "# parser.add_argument('--kernel_size', type=int, default=5,\n",
    "#                     help=\"Filter width of kernels. Has to be an odd number\")\n",
    "# parser.add_argument('--output_size', type=float, default=2.0,\n",
    "#                     help=\"Output duration\")\n",
    "# parser.add_argument('--strides', type=int, default=4,\n",
    "#                     help=\"Strides in Waveunet\")\n",
    "# parser.add_argument('--conv_type', type=str, default=\"gn\",\n",
    "#                     help=\"Type of convolution (normal, BN-normalised, GN-normalised): normal/bn/gn\")\n",
    "# parser.add_argument('--res', type=str, default=\"fixed\",\n",
    "#                     help=\"Resampling strategy: fixed sinc-based lowpass filtering or learned conv layer: fixed/learned\")\n",
    "# parser.add_argument('--separate', type=int, default=1,\n",
    "#                     help=\"Train separate model for each source (1) or only one (0)\")\n",
    "# parser.add_argument('--feature_growth', type=str, default=\"double\",\n",
    "#                     help=\"How the features in each layer should grow, either (add) the initial number of features each time, or multiply by 2 (double)\")\n",
    "\n",
    "# parser.add_argument('--input', type=str, default=os.path.join(\"audio_examples\", \"Cristina Vane - So Easy\", \"mix.mp3\"),\n",
    "#                     help=\"Path to input mixture to be separated\")\n",
    "# parser.add_argument('--output', type=str, default=None, help=\"Output path (same folder as input path if not set)\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "INSTRUMENTS = [\"bass\", \"drums\", \"other\", \"vocals\"]\n",
    "NUM_INSTRUMENTS = len(INSTRUMENTS)\n",
    "\n",
    "# MODEL\n",
    "num_features = [args.features*i for i in range(1, args.levels+1)] if args.feature_growth == \"add\" else \\\n",
    "               [args.features*2**i for i in range(0, args.levels)]\n",
    "target_outputs = int(args.output_size * args.sr)\n",
    "model = Waveunet(args.channels, num_features, args.channels, INSTRUMENTS, kernel_size=args.kernel_size,\n",
    "                 target_output_size=target_outputs, depth=args.depth, strides=args.strides,\n",
    "                 conv_type=args.conv_type, res=args.res, separate=args.separate)\n",
    "\n",
    "if args.cuda:\n",
    "    model = utils.DataParallel(model)\n",
    "    print(\"move model to gpu\")\n",
    "    model.cuda()\n",
    "\n",
    "print(\"Loading model from checkpoint \" + str(args.load_model))\n",
    "state = utils.load_model(model, None, args.load_model)\n",
    "\n",
    "preds = predict_song(args, args.input, model)\n",
    "\n",
    "output_folder = os.path.dirname(args.input) if args.output is None else args.output\n",
    "for inst in preds.keys():\n",
    "    utils.write_wav(os.path.join(output_folder, os.path.basename(args.input) + \"_\" + inst + \".wav\"), preds[inst], args.sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88409"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.shapes[\"output_frames\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4776 4776\n",
      "97961 93185\n",
      "9552\n"
     ]
    }
   ],
   "source": [
    "pad_front_context = model.shapes[\"output_start_frame\"]\n",
    "pad_back_context = model.shapes[\"input_frames\"] - model.shapes[\"output_end_frame\"]\n",
    "print(pad_front_context, pad_back_context)\n",
    "print(model.shapes[\"input_frames\"] , model.shapes[\"output_end_frame\"])\n",
    "print(model.shapes[\"input_frames\"] - model.shapes[\"output_frames\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1323000)\n",
      "(2, 1326135)\n",
      "(2, 1335687)\n",
      "9552\n"
     ]
    }
   ],
   "source": [
    "audio = mix_audio\n",
    "print(audio.shape)\n",
    "expected_outputs = audio.shape[1]\n",
    "\n",
    "# Pad input if it is not divisible in length by the frame shift number\n",
    "output_shift = model.shapes[\"output_frames\"]\n",
    "pad_back = audio.shape[1] % output_shift\n",
    "pad_back = 0 if pad_back == 0 else output_shift - pad_back\n",
    "if pad_back > 0:\n",
    "    audio = np.pad(audio, [(0,0), (0, pad_back)], mode=\"constant\", constant_values=0.0)\n",
    "print(audio.shape)\n",
    "target_outputs = audio.shape[1]\n",
    "outputs = {key: np.zeros(audio.shape, np.float32) for key in model.instruments}\n",
    "\n",
    "# Pad mixture across time at beginning and end so that neural network can make prediction at the beginning and end of signal\n",
    "pad_front_context = model.shapes[\"output_start_frame\"]\n",
    "pad_back_context = model.shapes[\"input_frames\"] - model.shapes[\"output_end_frame\"]\n",
    "audio = np.pad(audio, [(0,0), (pad_front_context, pad_back_context)], mode=\"constant\", constant_values=0.0)\n",
    "print(audio.shape)\n",
    "print(audio.shape[1]% output_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
